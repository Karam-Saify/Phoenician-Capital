{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQgpsytBJzZ8"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Colab: PDF crawler + downloader + manifest\n",
        "# Fix: Waits for download event after \"Download is starting\" exception to avoid race conditions\n",
        "# =========================\n",
        "\n",
        "!pip -q install beautifulsoup4 tqdm requests playwright nest_asyncio\n",
        "!playwright install --with-deps chromium\n",
        "\n",
        "import os, re, json, time, hashlib, asyncio\n",
        "import nest_asyncio\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import deque, Counter\n",
        "from urllib.parse import urlparse, urljoin, urldefrag, urlunparse, parse_qsl, urlencode\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "START_URL = \"https://www.technology1.com/company/investors/annual-reports\"\n",
        "MAX_DEPTH = 0\n",
        "MAX_PDFS  = 200\n",
        "MAX_PAGES = 200\n",
        "\n",
        "OUTPUT_DIR    = \"/content/pdfs\"\n",
        "MANIFEST_PATH = \"/content/manifest.json\"\n",
        "\n",
        "CRAWL_SAME_DOMAIN_ONLY = True\n",
        "HTML_ALLOWED_PREFIX    = \"/company/investors\"\n",
        "ALLOW_OFFDOMAIN_PDFS   = True\n",
        "\n",
        "REQUEST_TIMEOUT   = 30\n",
        "REQUEST_DELAY_SEC = 0.10\n",
        "VERIFY_TLS        = True\n",
        "\n",
        "DEBUG = True\n",
        "DEBUG_MAX_PAGES_PRINT = 2\n",
        "DEBUG_PDF_FIRST_N = 3  # print debug for first N PDF downloads\n",
        "\n",
        "BROWSER_HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "                  \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "}\n",
        "\n",
        "PDF_MAGIC = b\"%PDF\"\n",
        "TRACKING_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\"gclid\",\"fbclid\"}\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "def ensure_dir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def utc_now_iso() -> str:\n",
        "    return time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "\n",
        "def sha256_bytes(b: bytes) -> str:\n",
        "    return hashlib.sha256(b).hexdigest()\n",
        "\n",
        "def safe_filename(name: str, max_len: int = 180) -> str:\n",
        "    name = re.sub(r\"[^\\w\\-. ]+\", \"_\", name).strip()\n",
        "    if len(name) > max_len:\n",
        "        root, ext = os.path.splitext(name)\n",
        "        name = root[: max_len - len(ext) - 1] + \"_\" + ext\n",
        "    return name or \"document.pdf\"\n",
        "\n",
        "def normalize_url(url: str, base: str) -> str | None:\n",
        "    if not url:\n",
        "        return None\n",
        "    url = url.strip()\n",
        "    if url.startswith((\"mailto:\", \"tel:\", \"javascript:\", \"data:\")):\n",
        "        return None\n",
        "    abs_url = urljoin(base, url)\n",
        "    abs_url, _ = urldefrag(abs_url)\n",
        "    p = urlparse(abs_url)\n",
        "    if p.scheme not in (\"http\", \"https\"):\n",
        "        return None\n",
        "    q = [(k, v) for (k, v) in parse_qsl(p.query, keep_blank_values=True) if k not in TRACKING_PARAMS]\n",
        "    q.sort(key=lambda kv: (kv[0], kv[1]))\n",
        "    new_query = urlencode(q, doseq=True)\n",
        "    p2 = p._replace(scheme=p.scheme.lower(), netloc=p.netloc.lower(), query=new_query)\n",
        "    return urlunparse(p2)\n",
        "\n",
        "def looks_like_pdf(url: str) -> bool:\n",
        "    path = urlparse(url).path.lower()\n",
        "    return path.endswith(\".pdf\") or \"/__data/assets/pdf_file/\" in path or \".pdf\" in path\n",
        "\n",
        "def html_in_scope(url: str, start_parsed) -> bool:\n",
        "    p = urlparse(url)\n",
        "    if CRAWL_SAME_DOMAIN_ONLY and p.netloc.lower() != start_parsed.netloc.lower():\n",
        "        return False\n",
        "    if HTML_ALLOWED_PREFIX and not p.path.startswith(HTML_ALLOWED_PREFIX):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def pdf_in_scope(url: str, start_parsed) -> bool:\n",
        "    if ALLOW_OFFDOMAIN_PDFS:\n",
        "        return True\n",
        "    return urlparse(url).netloc.lower() == start_parsed.netloc.lower()\n",
        "\n",
        "def pick_filename_from_url(url: str) -> str:\n",
        "    path = urlparse(url).path\n",
        "    base = os.path.basename(path) or \"document.pdf\"\n",
        "    if not base.lower().endswith(\".pdf\"):\n",
        "        base += \".pdf\"\n",
        "    return safe_filename(base)\n",
        "\n",
        "def extract_links(soup: BeautifulSoup):\n",
        "    links = set()\n",
        "    for tag in soup.find_all(\"a\", href=True):\n",
        "        links.add(tag.get(\"href\"))\n",
        "    for tag in soup.find_all([\"iframe\", \"embed\"], src=True):\n",
        "        links.add(tag.get(\"src\"))\n",
        "    for tag in soup.find_all(\"object\", data=True):\n",
        "        links.add(tag.get(\"data\"))\n",
        "    for tag in soup.find_all(\"link\", href=True):\n",
        "        links.add(tag.get(\"href\"))\n",
        "    return [x for x in links if x]\n",
        "\n",
        "def is_cloudflare_interstitial(html: str) -> bool:\n",
        "    if not html:\n",
        "        return False\n",
        "    s = html.lower()\n",
        "    return (\"just a moment\" in s) and (\"cloudflare\" in s or \"/cdn-cgi/\" in s)\n",
        "\n",
        "def run_coro(coro):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    return loop.run_until_complete(coro)\n",
        "\n",
        "def make_requests_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    s.headers.update(BROWSER_HEADERS)\n",
        "    return s\n",
        "\n",
        "# =========================\n",
        "# Manifest record\n",
        "# =========================\n",
        "@dataclass\n",
        "class PdfRecord:\n",
        "    # required\n",
        "    source_url: str\n",
        "    local_path: str | None = None\n",
        "    downloaded_at: str | None = None\n",
        "    http_status: int | None = None\n",
        "    # optional but recommended\n",
        "    sha256: str | None = None\n",
        "    content_length: int | None = None\n",
        "    # diagnostics\n",
        "    discovered_on: str | None = None\n",
        "    final_url: str | None = None\n",
        "    content_type: str | None = None\n",
        "    status: str = \"pending\"   # downloaded|skipped|failed|pending\n",
        "    reason: str | None = None\n",
        "    duplicate_of: str | None = None\n",
        "\n",
        "# =========================\n",
        "# HTML Fetch (requests -> Playwright fallback)\n",
        "# =========================\n",
        "def fetch_html_requests(url: str, sess: requests.Session):\n",
        "    try:\n",
        "        resp = sess.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True, verify=VERIFY_TLS)\n",
        "        ct = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "        return True, resp.status_code, resp.url, ct, resp.text, \"requests\", None\n",
        "    except Exception as e:\n",
        "        return False, None, url, \"\", \"\", \"requests_error\", f\"{type(e).__name__}: {str(e)[:220]}\"\n",
        "\n",
        "async def fetch_html_playwright(url: str):\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                headless=True,\n",
        "                args=[\"--disable-dev-shm-usage\", \"--no-sandbox\", \"--disable-setuid-sandbox\"]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent=BROWSER_HEADERS[\"User-Agent\"],\n",
        "                locale=\"en-US\",\n",
        "                viewport={\"width\": 1280, \"height\": 800},\n",
        "                ignore_https_errors=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            resp = await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "            await page.wait_for_timeout(2000)\n",
        "            final_url = page.url\n",
        "            status = resp.status if resp is not None else 200\n",
        "            html = await page.content()\n",
        "            await context.close()\n",
        "            await browser.close()\n",
        "            return True, status, final_url, \"text/html\", html, \"playwright\", None\n",
        "    except PlaywrightTimeoutError as e:\n",
        "        return False, 408, url, \"text/html\", \"\", \"playwright_timeout\", f\"timeout: {str(e)[:220]}\"\n",
        "    except Exception as e:\n",
        "        return False, None, url, \"text/html\", \"\", \"playwright_error\", f\"{type(e).__name__}: {str(e)[:220]}\"\n",
        "\n",
        "def fetch_html(url: str, sess: requests.Session):\n",
        "    ok, status, final_url, ct, html, method, err = fetch_html_requests(url, sess)\n",
        "    if (status in (403, 429)) or is_cloudflare_interstitial(html):\n",
        "        return run_coro(fetch_html_playwright(url))\n",
        "    return ok, status, final_url, ct, html, method, err\n",
        "\n",
        "# =========================\n",
        "# Crawl\n",
        "# =========================\n",
        "def crawl_and_collect_pdfs(start_url: str):\n",
        "    start_parsed = urlparse(start_url)\n",
        "    visited_pages = set()\n",
        "    seen_links = set()\n",
        "    pdf_candidates: dict[str, PdfRecord] = {}\n",
        "\n",
        "    q = deque([(start_url, 0)])\n",
        "    sess = make_requests_session()\n",
        "    crawl_failures = []\n",
        "    debug_printed = 0\n",
        "\n",
        "    while q and len(visited_pages) < MAX_PAGES:\n",
        "        page_url, depth = q.popleft()\n",
        "        if depth > MAX_DEPTH or page_url in visited_pages:\n",
        "            continue\n",
        "\n",
        "        visited_pages.add(page_url)\n",
        "        time.sleep(REQUEST_DELAY_SEC)\n",
        "\n",
        "        ok, status, final_url, ct, html, method, err = fetch_html(page_url, sess)\n",
        "\n",
        "        if DEBUG and debug_printed < DEBUG_MAX_PAGES_PRINT:\n",
        "            debug_printed += 1\n",
        "            print(\"\\n=== CRAWL DEBUG ===\")\n",
        "            print(\"Requested:\", page_url)\n",
        "            print(\"Final URL:\", final_url)\n",
        "            print(\"Depth:\", depth)\n",
        "            print(\"OK:\", ok)\n",
        "            print(\"HTTP:\", status, f\"({method})\")\n",
        "            print(\"Content-Type:\", ct or \"text/html\")\n",
        "            print(\"HTML length:\", len(html or \"\"))\n",
        "            print(\"Cloudflare interstitial?:\", is_cloudflare_interstitial(html))\n",
        "            if err:\n",
        "                print(\"Error message:\", err)\n",
        "            snippet = (html or \"\")[:220].replace(\"\\n\", \" \")\n",
        "            print(\"Body snippet:\", snippet)\n",
        "            print(\"===================\")\n",
        "\n",
        "        if not ok or (status is not None and status >= 400):\n",
        "            reason = \"blocked_by_cloudflare\" if is_cloudflare_interstitial(html) else \"crawl_http_error\"\n",
        "            if method.startswith(\"playwright\") and err:\n",
        "                reason = \"playwright_failed\"\n",
        "            crawl_failures.append({\"url\": page_url, \"http_status\": status, \"reason\": reason, \"details\": err})\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        raw_links = extract_links(soup)\n",
        "\n",
        "        for href in raw_links:\n",
        "            norm = normalize_url(href, base=final_url)\n",
        "            if not norm or norm in seen_links:\n",
        "                continue\n",
        "            seen_links.add(norm)\n",
        "\n",
        "            if looks_like_pdf(norm):\n",
        "                if pdf_in_scope(norm, start_parsed):\n",
        "                    pdf_candidates.setdefault(norm, PdfRecord(source_url=norm, discovered_on=final_url))\n",
        "                continue\n",
        "\n",
        "            if depth < MAX_DEPTH and html_in_scope(norm, start_parsed):\n",
        "                q.append((norm, depth + 1))\n",
        "\n",
        "    return pdf_candidates, len(visited_pages), crawl_failures\n",
        "\n",
        "# =========================\n",
        "# Download PDFs via Playwright navigation (UPDATED FIX)\n",
        "# =========================\n",
        "async def download_pdfs_via_navigation(urls: list[str], pdf_candidates: dict[str, PdfRecord], output_dir: str):\n",
        "    ensure_dir(output_dir)\n",
        "\n",
        "    downloaded_by_final_url = set()\n",
        "    downloaded_by_hash: dict[str, str] = {}\n",
        "    results: list[PdfRecord] = []\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(\n",
        "            headless=True,\n",
        "            args=[\"--disable-dev-shm-usage\", \"--no-sandbox\", \"--disable-setuid-sandbox\"]\n",
        "        )\n",
        "        context = await browser.new_context(\n",
        "            user_agent=BROWSER_HEADERS[\"User-Agent\"],\n",
        "            locale=\"en-US\",\n",
        "            ignore_https_errors=True,\n",
        "            accept_downloads=True\n",
        "        )\n",
        "        page = await context.new_page()\n",
        "        page.set_default_navigation_timeout(60000)\n",
        "\n",
        "        # Land on START_URL first (get cookies/session).\n",
        "        try:\n",
        "            await page.goto(START_URL, wait_until=\"domcontentloaded\")\n",
        "            await page.wait_for_timeout(1200)\n",
        "        except Exception as e:\n",
        "            print(f\"Initial setup navigation warning: {e}\")\n",
        "\n",
        "        for idx, url in enumerate(tqdm(urls, desc=\"Downloading PDFs (browser navigation)\"), start=1):\n",
        "            rec = pdf_candidates[url]\n",
        "            rec.downloaded_at = utc_now_iso()\n",
        "\n",
        "            try:\n",
        "                # 1. Prepare to catch a download event\n",
        "                download_future = asyncio.Future()\n",
        "\n",
        "                def on_download(download):\n",
        "                    if not download_future.done():\n",
        "                        download_future.set_result(download)\n",
        "\n",
        "                page.on(\"download\", on_download)\n",
        "\n",
        "                # 2. Referer trick\n",
        "                try:\n",
        "                    await page.goto(START_URL, wait_until=\"domcontentloaded\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # 3. Go to the actual PDF URL\n",
        "                resp = None\n",
        "                browser_download = None\n",
        "\n",
        "                try:\n",
        "                    resp = await page.goto(url, wait_until=\"domcontentloaded\")\n",
        "                    await page.wait_for_timeout(300)\n",
        "                except Exception as e:\n",
        "                    if \"Download is starting\" in str(e):\n",
        "                        # Fix: If download is starting, we MUST wait for the future\n",
        "                        # because it might not be 'done' yet in the microtask queue.\n",
        "                        try:\n",
        "                            browser_download = await asyncio.wait_for(download_future, timeout=30.0)\n",
        "                        except asyncio.TimeoutError:\n",
        "                            rec.reason = \"Download event timed out (captured 'Download is starting' but no event)\"\n",
        "                    else:\n",
        "                        rec.reason = f\"Nav Error: {str(e)[:100]}\"\n",
        "\n",
        "                page.remove_listener(\"download\", on_download)\n",
        "\n",
        "                # 4. Handle results\n",
        "                content_body = b\"\"\n",
        "\n",
        "                # Check if we got a download via the future (either from exception block or concurrent success)\n",
        "                if not browser_download and download_future.done():\n",
        "                    browser_download = download_future.result()\n",
        "\n",
        "                if browser_download:\n",
        "                    try:\n",
        "                        # Wait for download to finish on disk\n",
        "                        dl_path = await browser_download.path()\n",
        "                        if dl_path:\n",
        "                            with open(dl_path, \"rb\") as f:\n",
        "                                content_body = f.read()\n",
        "                        rec.http_status = 200\n",
        "                        rec.final_url = browser_download.url\n",
        "                        rec.content_type = \"application/pdf\"\n",
        "                    except Exception as e:\n",
        "                        rec.reason = f\"Download file read error: {e}\"\n",
        "                        rec.status = \"failed\"\n",
        "                        results.append(rec)\n",
        "                        continue\n",
        "\n",
        "                # Otherwise, check if we have a standard page response (Inline PDF)\n",
        "                elif resp:\n",
        "                    rec.http_status = resp.status\n",
        "                    rec.final_url = page.url\n",
        "                    rec.content_type = (resp.headers.get(\"content-type\") or \"\").lower()\n",
        "\n",
        "                    if rec.http_status < 400:\n",
        "                        content_body = await resp.body()\n",
        "\n",
        "                # 5. Process the Content (Common Logic)\n",
        "                if not content_body:\n",
        "                    if not rec.reason:\n",
        "                        rec.reason = f\"No content retrieved (HTTP {rec.http_status})\"\n",
        "                    rec.status = \"failed\"\n",
        "                    results.append(rec)\n",
        "                    continue\n",
        "\n",
        "                # Magic bytes check\n",
        "                is_pdf_by_magic = content_body.lstrip().startswith(PDF_MAGIC)\n",
        "                is_pdf_by_header = \"application/pdf\" in (rec.content_type or \"\")\n",
        "\n",
        "                if not (is_pdf_by_magic or is_pdf_by_header):\n",
        "                    rec.status = \"skipped\"\n",
        "                    rec.reason = \"Not a PDF (content-type + magic-bytes failed)\"\n",
        "                    results.append(rec)\n",
        "                    continue\n",
        "\n",
        "                # Dedupe by Final URL\n",
        "                if rec.final_url in downloaded_by_final_url:\n",
        "                    rec.status = \"skipped\"\n",
        "                    rec.reason = \"Duplicate final URL\"\n",
        "                    results.append(rec)\n",
        "                    continue\n",
        "\n",
        "                rec.content_length = len(content_body)\n",
        "                rec.sha256 = sha256_bytes(content_body)\n",
        "\n",
        "                # Dedupe by Hash\n",
        "                if rec.sha256 in downloaded_by_hash:\n",
        "                    rec.status = \"skipped\"\n",
        "                    rec.reason = \"Duplicate content (sha256 match)\"\n",
        "                    rec.duplicate_of = downloaded_by_hash[rec.sha256]\n",
        "                    results.append(rec)\n",
        "                    continue\n",
        "\n",
        "                # Save file\n",
        "                fname = pick_filename_from_url(rec.final_url or url)\n",
        "                out_path = os.path.join(output_dir, fname)\n",
        "                if os.path.exists(out_path):\n",
        "                    root, ext = os.path.splitext(out_path)\n",
        "                    out_path = f\"{root}_{hashlib.md5((rec.final_url or url).encode()).hexdigest()[:8]}{ext}\"\n",
        "\n",
        "                with open(out_path, \"wb\") as f:\n",
        "                    f.write(content_body)\n",
        "\n",
        "                rec.local_path = out_path\n",
        "                rec.status = \"downloaded\"\n",
        "\n",
        "                downloaded_by_hash[rec.sha256] = out_path\n",
        "                downloaded_by_final_url.add(rec.final_url)\n",
        "\n",
        "                if DEBUG and idx <= DEBUG_PDF_FIRST_N:\n",
        "                    print(\"\\n=== PDF DOWNLOAD DEBUG ===\")\n",
        "                    print(\"Source:\", rec.source_url)\n",
        "                    print(\"Mode  :\", \"Attachment\" if browser_download else \"Inline\")\n",
        "                    print(\"Bytes :\", rec.content_length)\n",
        "                    print(\"SHA256:\", rec.sha256[:16], \"...\")\n",
        "                    print(\"==========================\")\n",
        "\n",
        "                results.append(rec)\n",
        "\n",
        "            except Exception as e:\n",
        "                rec.status = \"failed\"\n",
        "                rec.reason = f\"Exception: {type(e).__name__}: {str(e)[:120]}\"\n",
        "                results.append(rec)\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "\n",
        "    return results\n",
        "\n",
        "# =========================\n",
        "# Manifest + summary\n",
        "# =========================\n",
        "def write_manifest(path: str, start_url: str, pages_visited: int, pdf_candidates: dict, records: list[PdfRecord], crawl_failures):\n",
        "    manifest = {\n",
        "        \"run\": {\n",
        "            \"start_url\": start_url,\n",
        "            \"max_depth\": MAX_DEPTH,\n",
        "            \"max_pdfs\": MAX_PDFS,\n",
        "            \"pages_visited\": pages_visited,\n",
        "            \"pdfs_found\": len(pdf_candidates),\n",
        "            \"pdfs_downloaded\": sum(1 for r in records if r.status == \"downloaded\"),\n",
        "            \"download_failures\": sum(1 for r in records if r.status == \"failed\"),\n",
        "            \"crawl_failures\": len(crawl_failures),\n",
        "            \"timestamp_utc\": utc_now_iso(),\n",
        "        },\n",
        "        \"items\": [asdict(r) for r in records],\n",
        "        \"crawl_failures\": crawl_failures,\n",
        "    }\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
        "    return manifest\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "ensure_dir(OUTPUT_DIR)\n",
        "\n",
        "pdf_candidates, pages_visited, crawl_failures = crawl_and_collect_pdfs(START_URL)\n",
        "\n",
        "print(\"\\n=== PDF CANDIDATES DEBUG ===\")\n",
        "print(\"Total candidates:\", len(pdf_candidates))\n",
        "for i, (u, rec) in enumerate(list(pdf_candidates.items())[:12], start=1):\n",
        "    print(f\"{i}. {u} (discovered_on={rec.discovered_on})\")\n",
        "\n",
        "urls = list(pdf_candidates.keys())[:MAX_PDFS]\n",
        "records = run_coro(download_pdfs_via_navigation(urls, pdf_candidates, OUTPUT_DIR))\n",
        "manifest = write_manifest(MANIFEST_PATH, START_URL, pages_visited, pdf_candidates, records, crawl_failures)\n",
        "\n",
        "# End-of-run summary\n",
        "download_failures = [r for r in records if r.status == \"failed\"]\n",
        "crawl_fail_counts = Counter((x.get(\"reason\", \"unknown\") for x in crawl_failures))\n",
        "dl_fail_counts    = Counter(((r.reason or \"unknown\") for r in download_failures))\n",
        "\n",
        "print(\"\\n=== End-of-Run Summary ===\")\n",
        "print(f\"Pages visited:     {pages_visited}\")\n",
        "print(f\"PDFs found:        {len(pdf_candidates)}\")\n",
        "print(f\"PDFs downloaded:   {manifest['run']['pdfs_downloaded']}\")\n",
        "print(f\"Failures:          {manifest['run']['crawl_failures'] + manifest['run']['download_failures']}\")\n",
        "\n",
        "if crawl_failures:\n",
        "    print(\"Crawl failures (top reasons):\")\n",
        "    for reason, cnt in crawl_fail_counts.most_common(5):\n",
        "        print(f\"  - {cnt}x {reason}\")\n",
        "\n",
        "if download_failures:\n",
        "    print(\"Download failures (top reasons):\")\n",
        "    for reason, cnt in dl_fail_counts.most_common(5):\n",
        "        print(f\"  - {cnt}x {reason}\")\n",
        "\n",
        "print(f\"\\nOutput folder: {OUTPUT_DIR}\")\n",
        "print(f\"Manifest:      {MANIFEST_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pypdf2 rank-bm25 transformers torch -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "PDF_FOLDER = \"/content/pdfs\"\n",
        "INDEX_FOLDER = \"/content/data/index\"\n",
        "CHUNK_SIZE = 600\n",
        "CHUNK_OVERLAP = 150\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, text: str, pdf: str, page: int):\n",
        "        self.text = text\n",
        "        self.pdf = pdf\n",
        "        self.page = page\n",
        "\n",
        "class AIRAGChatbot:\n",
        "    def __init__(self):\n",
        "        self.documents: List[Document] = []\n",
        "        self.bm25 = None\n",
        "        self.tokenized_corpus = []\n",
        "        self.qa_model = None\n",
        "\n",
        "    def load_qa_model(self):\n",
        "        \"\"\"Load question-answering model\"\"\"\n",
        "        print(\"ðŸ¤– Loading AI model (this may take a minute)...\")\n",
        "        # Using a small, fast QA model that works well in Colab\n",
        "        self.qa_model = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=\"deepset/roberta-base-squad2\",\n",
        "            device=-1  # CPU\n",
        "        )\n",
        "        print(\"âœ“ AI model loaded\")\n",
        "\n",
        "    def extract_text_from_pdfs(self, pdf_folder: str):\n",
        "        \"\"\"Extract text from all PDFs with page association\"\"\"\n",
        "        print(f\"ðŸ“š Reading PDFs from {pdf_folder}...\")\n",
        "\n",
        "        if not os.path.exists(pdf_folder):\n",
        "            raise ValueError(f\"PDF folder not found: {pdf_folder}\")\n",
        "\n",
        "        pdf_files = list(Path(pdf_folder).glob(\"*.pdf\"))\n",
        "        if not pdf_files:\n",
        "            raise ValueError(f\"No PDF files found in {pdf_folder}\")\n",
        "\n",
        "        for pdf_path in pdf_files:\n",
        "            try:\n",
        "                reader = PdfReader(str(pdf_path))\n",
        "                pdf_name = pdf_path.name\n",
        "\n",
        "                for page_num, page in enumerate(reader.pages, start=1):\n",
        "                    text = page.extract_text()\n",
        "                    if text.strip():\n",
        "                        chunks = self.chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                        for chunk in chunks:\n",
        "                            self.documents.append(Document(\n",
        "                                text=chunk,\n",
        "                                pdf=pdf_name,\n",
        "                                page=page_num\n",
        "                            ))\n",
        "\n",
        "                print(f\"âœ“ Processed: {pdf_name} ({len(reader.pages)} pages)\")\n",
        "            except Exception as e:\n",
        "                print(f\"âœ— Error processing {pdf_path.name}: {e}\")\n",
        "\n",
        "        print(f\"âœ“ Total chunks extracted: {len(self.documents)}\")\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            if chunk.strip():\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        return chunks if chunks else [text]\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build BM25 index\"\"\"\n",
        "        print(\"ðŸ”¨ Building BM25 index...\")\n",
        "\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"No documents to index!\")\n",
        "\n",
        "        self.tokenized_corpus = [\n",
        "            doc.text.lower().split() for doc in self.documents\n",
        "        ]\n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "\n",
        "        print(f\"âœ“ Index built with {len(self.documents)} chunks\")\n",
        "\n",
        "    def save_index(self, index_folder: str):\n",
        "        \"\"\"Persist index to disk\"\"\"\n",
        "        print(f\"ðŸ’¾ Saving index to {index_folder}...\")\n",
        "\n",
        "        os.makedirs(index_folder, exist_ok=True)\n",
        "\n",
        "        docs_data = [\n",
        "            {\"text\": doc.text, \"pdf\": doc.pdf, \"page\": doc.page}\n",
        "            for doc in self.documents\n",
        "        ]\n",
        "\n",
        "        with open(f\"{index_folder}/documents.json\", \"w\") as f:\n",
        "            json.dump(docs_data, f)\n",
        "\n",
        "        with open(f\"{index_folder}/bm25.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.bm25, f)\n",
        "\n",
        "        print(\"âœ“ Index saved successfully\")\n",
        "\n",
        "    def load_index(self, index_folder: str):\n",
        "        \"\"\"Load index from disk\"\"\"\n",
        "        print(f\"ðŸ“‚ Loading index from {index_folder}...\")\n",
        "\n",
        "        with open(f\"{index_folder}/documents.json\", \"r\") as f:\n",
        "            docs_data = json.load(f)\n",
        "\n",
        "        self.documents = [\n",
        "            Document(d[\"text\"], d[\"pdf\"], d[\"page\"]) for d in docs_data\n",
        "        ]\n",
        "\n",
        "        with open(f\"{index_folder}/bm25.pkl\", \"rb\") as f:\n",
        "            self.bm25 = pickle.load(f)\n",
        "\n",
        "        self.tokenized_corpus = [\n",
        "            doc.text.lower().split() for doc in self.documents\n",
        "        ]\n",
        "\n",
        "        print(f\"âœ“ Index loaded with {len(self.documents)} chunks\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 10) -> List[Dict]:\n",
        "        \"\"\"Retrieve top-k relevant documents\"\"\"\n",
        "        if not self.bm25:\n",
        "            raise ValueError(\"Index not built or loaded!\")\n",
        "\n",
        "        tokenized_query = query.lower().split()\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            doc = self.documents[idx]\n",
        "            results.append({\n",
        "                \"pdf\": doc.pdf,\n",
        "                \"page\": doc.page,\n",
        "                \"snippet\": doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text,\n",
        "                \"full_text\": doc.text,\n",
        "                \"score\": scores[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def answer_question(self, question: str, top_k: int = 10) -> Dict:\n",
        "        \"\"\"Answer question using AI model\"\"\"\n",
        "\n",
        "        if not self.qa_model:\n",
        "            self.load_qa_model()\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        sources = self.retrieve(question, top_k)\n",
        "\n",
        "        if not sources:\n",
        "            return {\n",
        "                \"answer\": \"I couldn't find relevant information to answer this question.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        # Combine top sources as context\n",
        "        context = \"\\n\\n\".join([src[\"full_text\"] for src in sources[:5]])\n",
        "\n",
        "        # Truncate context if too long (model limit is ~512 tokens)\n",
        "        max_context_length = 2000\n",
        "        if len(context) > max_context_length:\n",
        "            context = context[:max_context_length]\n",
        "\n",
        "        try:\n",
        "            # Use AI model to extract answer\n",
        "            result = self.qa_model(\n",
        "                question=question,\n",
        "                context=context,\n",
        "                max_answer_len=200,\n",
        "                handle_impossible_answer=True\n",
        "            )\n",
        "\n",
        "            answer = result['answer']\n",
        "            confidence = result['score']\n",
        "\n",
        "            # If confidence is too low, return top chunk instead\n",
        "            if confidence < 0.1:\n",
        "                answer = sources[0][\"full_text\"][:400] + \"...\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"AI model error: {e}\")\n",
        "            answer = sources[0][\"full_text\"][:400] + \"...\"\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"pdf\": src[\"pdf\"],\n",
        "                    \"page\": src[\"page\"],\n",
        "                    \"snippet\": src[\"snippet\"]\n",
        "                }\n",
        "                for src in sources[:3]\n",
        "            ]\n",
        "        }\n",
        "\n",
        "# Initialize chatbot\n",
        "print(\"ðŸš€ Initializing AI-Powered RAG Chatbot...\")\n",
        "chatbot = AIRAGChatbot()\n",
        "\n",
        "# Ingest PDFs and build index\n",
        "try:\n",
        "    if os.path.exists(f\"{INDEX_FOLDER}/bm25.pkl\"):\n",
        "        chatbot.load_index(INDEX_FOLDER)\n",
        "    else:\n",
        "        chatbot.extract_text_from_pdfs(PDF_FOLDER)\n",
        "        chatbot.build_index()\n",
        "        chatbot.save_index(INDEX_FOLDER)\n",
        "\n",
        "    print(f\"\\nâœ… Chatbot ready! Indexed {len(chatbot.documents)} chunks\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    raise\n",
        "\n",
        "def ask(question: str):\n",
        "    \"\"\"Ask any question - AI will understand and answer\"\"\"\n",
        "    result = chatbot.answer_question(question)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"â“ {question}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nðŸ’¡ {result['answer']}\\n\")\n",
        "\n",
        "    if result['sources']:\n",
        "        print(f\"ðŸ“š Sources:\")\n",
        "        for i, src in enumerate(result['sources'], 1):\n",
        "            print(f\"  [{i}] {src['pdf']} (Page {src['page']})\")\n",
        "\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"ðŸ” Ask ANY question - the AI will understand!\")\n",
        "print(\"\\nExamples:\")\n",
        "print(\"  ask('What is the revenue?')\")\n",
        "print(\"  ask('How much profit did they make?')\")\n",
        "print(\"  ask('Who runs the company?')\")\n",
        "print(\"  ask('What is their growth strategy?')\")\n",
        "print(\"  ask('What markets do they serve?')\")"
      ],
      "metadata": {
        "id": "VpiCejjqJ1y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('What is TechnologyOne total revenue for FY24?')\n",
        "ask('What was the net profit?')\n",
        "ask('Who is the CEO?')"
      ],
      "metadata": {
        "id": "j9R6ZQMfJ_VC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}